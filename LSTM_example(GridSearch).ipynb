{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense # type: ignore\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"sample_DATA_LSTM.csv\")\n",
    "\n",
    "# Convert TimeStamp to datetime and set as index\n",
    "data['TimeStamp'] = pd.to_datetime(data['TimeStamp'])\n",
    "data.set_index('TimeStamp', inplace=True)\n",
    "\n",
    "# Ensure timestamps are complete with 3-minute intervals\n",
    "data = data.resample('3T').mean()\n",
    "\n",
    "\n",
    "\n",
    "# Handle missing values (interpolation)\n",
    "data.interpolate(method='linear', inplace=True)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "# for simflicity, we will only use the first 5000 rows\n",
    "data = data.head(5000)\n",
    "\n",
    "## Create a dataset with the first columns to train on\n",
    "data_train = data[data.columns[0:1]]\n",
    "# Create a dataset that the model would predict the model on \n",
    "data_predict = data[data.columns[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function that create Dataset of the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the dataset for LSTM \n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        a = dataset[i:(i + time_step), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSerch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: A01a-Z0\n",
      "13:14:13 - Processing combination 1/36 for column: A01a-Z0\n",
      "\t13:14:13 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 5, 'units': 50}\n",
      "32/32 [==============================] - 4s 7ms/step\n",
      "\t13:14:43 -A01a-Z0 error (MAE): 0.4331774500150315\n",
      "13:14:43 - Processing combination 2/36 for column: A01a-Z0\n",
      "\t13:14:43 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 5, 'units': 100}\n",
      "32/32 [==============================] - 1s 6ms/step\n",
      "\t13:15:18 -A01a-Z0 error (MAE): 0.4247947005151023\n",
      "13:15:18 - Processing combination 3/36 for column: A01a-Z0\n",
      "\t13:15:18 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 5, 'units': 150}\n",
      "32/32 [==============================] - 2s 12ms/step\n",
      "\t13:16:08 -A01a-Z0 error (MAE): 0.6030754382557553\n",
      "13:16:08 - Processing combination 4/36 for column: A01a-Z0\n",
      "\t13:16:08 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 10, 'units': 50}\n",
      "31/31 [==============================] - 2s 9ms/step\n",
      "\t13:16:40 -A01a-Z0 error (MAE): 0.38287812159939577\n",
      "13:16:40 - Processing combination 5/36 for column: A01a-Z0\n",
      "\t13:16:40 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 10, 'units': 100}\n",
      "31/31 [==============================] - 3s 17ms/step\n",
      "\t13:17:28 -A01a-Z0 error (MAE): 0.42732768135665605\n",
      "13:17:28 - Processing combination 6/36 for column: A01a-Z0\n",
      "\t13:17:28 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 10, 'units': 150}\n",
      "31/31 [==============================] - 5s 29ms/step\n",
      "\t13:18:44 -A01a-Z0 error (MAE): 0.2876003814757357\n",
      "13:18:44 - Processing combination 7/36 for column: A01a-Z0\n",
      "\t13:18:44 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 15, 'units': 50}\n",
      "31/31 [==============================] - 1s 10ms/step\n",
      "\t13:19:19 -A01a-Z0 error (MAE): 0.3565310587942167\n",
      "13:19:19 - Processing combination 8/36 for column: A01a-Z0\n",
      "\t13:19:19 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 15, 'units': 100}\n",
      "31/31 [==============================] - 2s 18ms/step\n",
      "\t13:20:24 -A01a-Z0 error (MAE): 0.3001797036590055\n",
      "13:20:24 - Processing combination 9/36 for column: A01a-Z0\n",
      "\t13:20:24 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 10, 'time_step': 15, 'units': 150}\n",
      "31/31 [==============================] - 3s 39ms/step\n",
      "\t13:21:49 -A01a-Z0 error (MAE): 0.26770680115780976\n",
      "13:21:49 - Processing combination 10/36 for column: A01a-Z0\n",
      "\t13:21:49 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 20, 'time_step': 5, 'units': 50}\n",
      "32/32 [==============================] - 1s 4ms/step\n",
      "\t13:22:26 -A01a-Z0 error (MAE): 0.32699720394203613\n",
      "13:22:26 - Processing combination 11/36 for column: A01a-Z0\n",
      "\t13:22:26 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 20, 'time_step': 5, 'units': 100}\n",
      "32/32 [==============================] - 1s 7ms/step\n",
      "\t13:23:14 -A01a-Z0 error (MAE): 0.26125458715427324\n",
      "13:23:14 - Processing combination 12/36 for column: A01a-Z0\n",
      "\t13:23:14 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 20, 'time_step': 5, 'units': 150}\n",
      "32/32 [==============================] - 2s 17ms/step\n",
      "\t13:24:44 -A01a-Z0 error (MAE): 0.2142978021940235\n",
      "13:24:44 - Processing combination 13/36 for column: A01a-Z0\n",
      "\t13:24:44 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 20, 'time_step': 10, 'units': 50}\n",
      "31/31 [==============================] - 2s 9ms/step\n",
      "\t13:25:41 -A01a-Z0 error (MAE): 0.19728470081278404\n",
      "13:25:41 - Processing combination 14/36 for column: A01a-Z0\n",
      "\t13:25:41 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 20, 'time_step': 10, 'units': 100}\n",
      "31/31 [==============================] - 2s 15ms/step\n",
      "\t13:27:43 -A01a-Z0 error (MAE): 0.40493906766253934\n",
      "13:27:43 - Processing combination 15/36 for column: A01a-Z0\n",
      "\t13:27:43 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 20, 'time_step': 10, 'units': 150}\n",
      "31/31 [==============================] - 2s 25ms/step\n",
      "\t13:30:33 -A01a-Z0 error (MAE): 0.2304543740948179\n",
      "13:30:33 - Processing combination 16/36 for column: A01a-Z0\n",
      "\t13:30:33 - Training A01a-Z0 with hyperparameters: {'batch_size': 32, 'epochs': 20, 'time_step': 15, 'units': 50}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid for the LSTM model\n",
    "# 'param_grid' defines a grid of hyperparameters for the LSTM model.\n",
    "# It contains the following keys:\n",
    "# - 'units': Number of units (LSTM cells) in each LSTM layer. We test one value: 50.\n",
    "# - 'batch_size': The number of training samples processed before the model updates its weights. We test batch size of 32.\n",
    "# - 'epochs': Number of complete passes through the training dataset. We test 10 and 20 epochs.\n",
    "# - 'time_step': The number of previous time steps (look-back window) used to predict the next time step. We test 5 and 10 time steps.\n",
    "#   - If each index is in 3-minute intervals:\n",
    "#     - 5 time steps = 15 minutes.\n",
    "#     - 10 time steps = 30 minutes.\n",
    "\n",
    "param_grid = {\n",
    "    'units': [50, 100, 150],   # Test 50, 100, and 150 units\n",
    "    'batch_size': [32, 64],    # Test with batch sizes of 32 and 64\n",
    "    'epochs': [10, 20],        # Test with 10 and 20 epochs\n",
    "    'time_step': [5, 10, 15]       # Test with 5 and 10 time steps\n",
    "}\n",
    "# there are 3*2*2*3 = 36 combinations of hyperparameters to test\n",
    "\n",
    "# calculate the number of combinations based on the hyperparameter grid\n",
    "num_combinations = len(param_grid['units']) * len(param_grid['batch_size']) * len(param_grid['epochs']) * len(param_grid['time_step'])\n",
    "\n",
    "\n",
    "# Function to create dataset (X, y) based on time steps\n",
    "def create_dataset(data, time_step):\n",
    "    \"\"\"\n",
    "    Creates the dataset X, y based on the number of time steps (look-back window).\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The scaled data (numpy array).\n",
    "    - time_step: The number of time steps (look-back window) to use for prediction.\n",
    "    \n",
    "    Returns:\n",
    "    - X: Input features (previous time steps).\n",
    "    - y: Target values (next time step).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        X.append(data[i:(i + time_step), 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to train and evaluate the model with specific parameters\n",
    "def train_and_evaluate(params, column, scaled_data, scaler):\n",
    "    \"\"\"\n",
    "    Train and evaluate the LSTM model with specific hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - params: Dictionary containing the current hyperparameters being tested.\n",
    "    - column: The name of the column (time series) being processed.\n",
    "    - scaled_data: Scaled data used for training and testing.\n",
    "    - scaler: MinMaxScaler instance used for scaling and inverse transformation.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing the parameters, error (MAE), and training history.\n",
    "    \"\"\"\n",
    "    # Print the current combination of hyperparameters with time logging format (HH:MM:SS)\n",
    "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"\\t{now} - Training {column} with hyperparameters: {params}\")\n",
    "    \n",
    "    # Extract the time_step from params\n",
    "    time_step = params['time_step']\n",
    "    \n",
    "    # Create the dataset using the time_step\n",
    "    X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "    # Split the data into training and testing datasets\n",
    "    train_size = int(len(scaled_data) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # Reshape data to be in the format [samples, time steps, features] required by LSTM\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    # Build the LSTM model based on current hyperparameters\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['units'], return_sequences=True, input_shape=(time_step, 1)))  # LSTM layer 1\n",
    "    model.add(LSTM(params['units'], return_sequences=True))                             # LSTM layer 2\n",
    "    model.add(LSTM(params['units']))                                                    # LSTM layer 3 (no return sequences)\n",
    "    model.add(Dense(1))                                                                 # Dense layer (output layer)\n",
    "    \n",
    "    # Compile the model using mean squared error loss and Adam optimizer\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Train the model with given epochs and batch size\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                        epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n",
    "\n",
    "    # Predict on the test data\n",
    "    test_predict = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the predictions and actual values to their original scale\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    y_test = scaler.inverse_transform([y_test])\n",
    "\n",
    "    # Calculate prediction error (difference between predictions and actual values)\n",
    "    error = np.mean(np.abs(test_predict.flatten() - y_test.flatten()))  # Use mean absolute error (MAE) as the evaluation metric\n",
    "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"\\t{now} -{column} error (MAE): {error}\")\n",
    "    # Return the results: parameters, error, and history\n",
    "    return {'params': params, 'error': error, 'history': history.history, \"model\": model}\n",
    "\n",
    "# Function to perform grid search on the first column of the data\n",
    "def grid_search(data):\n",
    "    \"\"\"\n",
    "    Perform grid search over the hyperparameter grid for the LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the time series data.\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: Dictionary containing errors for each hyperparameter combination.\n",
    "    - errors: Dictionary of MAE errors for each hyperparameter combination.\n",
    "    - histories: Dictionary containing training histories for each hyperparameter combination.\n",
    "    - best_params: The hyperparameters that achieved the lowest MAE.\n",
    "    - best_error: The lowest validation MAE.\n",
    "    \"\"\"\n",
    "    # Prepare to store results\n",
    "    predictions = {}\n",
    "    errors = {}\n",
    "    histories = {}\n",
    "    best_params = None\n",
    "    best_error = float('inf')  # Initialize with a very high error value\n",
    "\n",
    "    combination_counter = 0  # Initialize a counter for the hyperparameter combinations\n",
    "\n",
    "    # Loop over the first column only\n",
    "    for column in data.columns[:]:  # Restricting to the first column\n",
    "        print(f\"Processing column: {column}\")\n",
    "        # Scale the data for the column\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(data[[column]])\n",
    "\n",
    "        # Loop over each combination of hyperparameters in the grid\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            combination_counter += 1  # Increment the counter for each combination\n",
    "            now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"{now} - Processing combination {combination_counter}/{num_combinations} for column: {column}\")\n",
    "            \n",
    "            # Train and evaluate the model with the current hyperparameters\n",
    "            result = train_and_evaluate(params, column, scaled_data, scaler)\n",
    "\n",
    "            # Store predictions, errors, and training histories\n",
    "            predictions[(column, str(result['params']))] = result['error']\n",
    "            errors[(column, str(result['params']))] = result['error']\n",
    "            histories[(column, str(result['params']))] = result['history']\n",
    "            \n",
    "            # Check if the current result is better (i.e., has a lower error) than the previous best\n",
    "            if result['error'] < best_error:\n",
    "                best_error = result['error']\n",
    "                best_params = result['params']\n",
    "                best_model = result[\"model\"]\n",
    "\n",
    "    # Return predictions, errors, histories, and the best hyperparameters found\n",
    "    return predictions, errors, histories, best_params, best_error, best_model\n",
    "\n",
    "# Run the grid search and get the best hyperparameters for the first column\n",
    "predictions, errors, histories, best_params, best_error, best_model = grid_search(data_train)\n",
    "\n",
    "# Print the best hyperparameters and their corresponding error\n",
    "print(f\"\\n\\nBest hyperparameters: {best_params}\")\n",
    "print(f\"Best validation error (MAE): {best_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  learning curves from histories\n",
    "### The `histories` object contains the training and validation metrics (such as loss or accuracy) over epochs for each hyperparameter combination at was tested during the grid search.\n",
    "###  This is useful for analyzing how the model performed over time and for visualizing how the loss (or other metrics) decreased as the model trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder():\n",
    "    \"\"\"\n",
    "    create a folder that stores the learning curves for each hyperparameter set\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists('learning_curves'):\n",
    "        os.makedirs('learning_curves')\n",
    "    return None\n",
    "\n",
    "# Loop through the histories dictionary to plot the training and validation loss\n",
    "token = 0\n",
    "for param_set, history in histories.items():\n",
    "    # Extract training and validation loss\n",
    "    train_loss = history['loss']\n",
    "    val_loss = history.get('val_loss', [])  # Validation loss (if available)\n",
    "\n",
    "    # Create a new figure for each hyperparameter set\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the training loss\n",
    "    plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "\n",
    "    # Plot the validation loss if available\n",
    "    if val_loss:\n",
    "        plt.plot(val_loss, label='Validation Loss', color='orange')\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title(f\"Learning Curves for Params: {param_set}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    ## save the img \n",
    "    create_folder()\n",
    "    plt.savefig(f'learning_curves/{token}_learning_curve.png')\n",
    "    token += 1\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the columns based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store predictions\n",
    "predictions = {}\n",
    "time_step = best_params['time_step']\n",
    "# Loop through each column in the dataset\n",
    "for column in data.columns[1:]:\n",
    "    print(f\"Analayzing for column: {column}\")\n",
    "    # Scale the data for this column individually\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(data[[column]])  # Scaling each column individually\n",
    "    \n",
    "    # Create dataset for this column\n",
    "    X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "    # Split into train and test sets (80% train, 20% test)\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # Reshape X to be in the form (samples, time_step, features)\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    # Predict using the best_model\n",
    "    print(f\"Stared Training for {column}\")\n",
    "    train_predict = best_model.predict(X_train)\n",
    "    print(f\"Stared Testing for {column}\")\n",
    "    test_predict = best_model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the predictions and actual values back to original scale\n",
    "    train_predict = scaler.inverse_transform(train_predict)\n",
    "    test_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Store predictions for each column\n",
    "    predictions[column] = {\n",
    "        'train_predict': train_predict,\n",
    "        'test_predict': test_predict,\n",
    "        'y_test_rescaled': y_test_rescaled\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now plot the results using the structure you provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns[1:]:\n",
    "    # Extract train/test predictions and true test values\n",
    "    train_predict = predictions[column]['train_predict']\n",
    "    test_predict = predictions[column]['test_predict']\n",
    "    y_test_rescaled = predictions[column]['y_test_rescaled']\n",
    "\n",
    "    # Calculate errors for test data (MAE)\n",
    "    errors = np.abs(test_predict.flatten() - y_test_rescaled.flatten())\n",
    "\n",
    "    # Create a figure with 1 row and 2 columns\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Predictions\", \"Errors\"))\n",
    "\n",
    "    # Plot train predictions\n",
    "    fig.add_trace(go.Scatter(x=data.index[:len(train_predict)], y=train_predict.flatten(), \n",
    "                             mode='lines', name=f'Train Predictions {column}'), row=1, col=1)\n",
    "    \n",
    "    # Plot test predictions\n",
    "    fig.add_trace(go.Scatter(x=data.index[len(train_predict):], y=test_predict.flatten(), \n",
    "                             mode='lines', name=f'Test Predictions {column}'), row=1, col=1)\n",
    "    \n",
    "    # Plot true values\n",
    "    fig.add_trace(go.Scatter(x=data.index, y=data[column], \n",
    "                             mode='lines', name=f'True Values {column}'), row=1, col=1)\n",
    "\n",
    "    # Plot the errors in the second subplot (right side)\n",
    "    fig.add_trace(go.Scatter(x=data.index[len(train_predict):], y=errors, \n",
    "                             mode='lines', name=f'Errors {column}'), row=1, col=2)\n",
    "\n",
    "    # Update layout for the figure\n",
    "    fig.update_layout(title=f'LSTM Predictions and Errors for {column}',\n",
    "                      height=800, width=1600,\n",
    "                      xaxis_title=\"Time\", \n",
    "                      yaxis_title=\"Values\")\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
